# Practical Machine Learning Project

## Overview
Human Activity Recognition (HAR) is coming out as a new field where wearable devices are  commonly
used to quantify performances. 
In the present study is considered the qualitative aspect.
The focus of the experiment is, specifically, in how well weight lifting exercises are performed.
Each of 6 partecipants in the experiment had various accelerometer data collected from devices 
and were asked to perform barbell lifts correctly and incorrectly in 5 different ways.
The goal is to predict the manner in which the participants completed the excercise, classified using the "classe" variable.

## Getting data

Primarily is performed the downloading of the training and test datasets in csv format.

```{r getData, echo=TRUE, cache=TRUE}


url<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"  
    
pmltraining <- "pml-training.csv"

if(!file.exists(pmltraining)) {

      download(url,destfile=pmltraining, mode="wb")          
}

url<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
    
pmltesting <- "pml-testing.csv"

if (!file.exists(pmltesting)){

     download(url,destfile=pmltesting, mode="wb") 

}


```
## Cleaning data

In order to avoid considering useless data, column variables containing null values are removed
and the first seven columns as they clearly are not related to accelerometer data.
This operation remove 100 useless predictors, the final datasets have both 53 columns,52 predictors and the last variable "classe".

```{r cleanData, echo=TRUE, cache=TRUE}

pmltrain <- read.csv("pml-training.csv", header = TRUE, na.strings=c("","NA"))
pmltest <- read.csv("pml-testing.csv", header = TRUE, na.strings=c("","NA"))

dim(pmltrain)
dim(pmltest)

summary(pmltrain$classe)

pmltrain <- pmltrain[, colSums(is.na(pmltrain)) == 0] 
pmltest <- pmltest[, colSums(is.na(pmltest)) == 0]

pmltrain <- pmltrain[ , -(1:7)]
pmltest <- pmltest[ , -(1:7)]

dim(pmltrain)
dim(pmltest)

```
## Partitioning data
The original cleaned training dataset is partitioned into a training and a testing subsets to be used for training the models and estimating errors. The data is split into 70% training and 30% testing.

```{r partData, echo=TRUE, cache=TRUE}

library(caret)

set.seed(12345) 

trainset <- createDataPartition(pmltrain$classe, p = 0.7, list = FALSE)
traindata <- pmltrain[trainset, ]
testdata <- pmltrain[-trainset, ]

```

## Building the Model

### Classification tree modeling
Since the problem is related to classification, in the first instance it is considered a classification tree model on the training set to determine the accuracy of that model.
Data transformation is evaluated less important in a non-linear model so it is avoided any variables transformation or preprocessing.

```{r classTreeModel, echo=TRUE, cache=TRUE}

control <- trainControl(method = "cv", number = 5)

classTreeFit<-train(classe~.,method="rpart",data=traindata,trControl = control)

classTreePredict<-predict(classTreeFit$finalModel,testdata, type = "class")

library(rpart.plot)

fancyRpartPlot(classTreeFit$finalModel)

(accuracyClassTree <- confusionMatrix(classTreePredict,testdata$classe))

accuracyClassTree$overall[1]


```
It can be seen from the evaluation shown above that the accuracy of the model is quite low. 
An accuracy of 0.49% giving an out-of-sample error of 0.51%.
A more appropriate approach, due to the nature of the problem, seems to be a random forest modeling with cross validation.

### Random forest modeling

A random forest modeling is performed using a k=5 k-fold cross validation and is evaluated the confusion matrix on test data to find the error.
Due to performance computation issues (as well as the choice to avoid the default k=10) it is used parallel processing.


```{r randomForestModel, echo=TRUE, cache=TRUE}

library(doParallel)
library(randomForest)

cl <- makeCluster(detectCores()) 
registerDoParallel(cl) 

if (!exists("randomForestFit")) {
  
  ctrl <- trainControl(method = "cv", number = 5, allowParallel = TRUE)
  randomForestFit <- train(classe ~ .,  method="rf", data=traindata,trControl=ctrl, 
                             verbose=TRUE,allowParallel=TRUE)

  
  
}


randomForestPredict<-predict(randomForestFit$finalModel,testdata, type="class")

(accuracyRandomForest <- confusionMatrix(randomForestPredict,testdata$classe))

accuracyRandomForest$overall[1]

```
Accuracy is 0.99%, so the related out-of-sample error is 0.01%.

## Final prediction
Due to the very good result of the accuracy of the last modeling (as known from the theory for problems of this kind), appears to be irrelevant trying others algorithms.
The final prediction is performed using Random Forest modeling.

```{r final, echo=TRUE, cache=TRUE}

(finaltest<-predict(randomForestFit$finalModel,pmltest,type="class"))

```
## Submission
The code below, provided with the assignment, creates a .txt file for each of the 20 samples and has the predicted "classe" variable (A,B,C,D,E) for that sample.

```{r submission, echo=TRUE, cache=TRUE}

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(finaltest)

```
